import scrapy

class TestCategoriesSpider(scrapy.Spider):
    name = "test_categories"
    allowed_domains = ["fandom.com"]

    # ğŸ”¹ Liste de toutes les pages de catÃ©gories que tu veux tester
    start_urls = [
	"https://obluda.fandom.com/wiki/Category:Characters",
	"https://haikyuu.fandom.com/wiki/Category:Characters",
	"https://nana.fandom.com/wiki/Category:Characters",
	"https://dr-stone.fandom.com/wiki/Category:Characters",
	"https://madeinabyss.fandom.com/wiki/Category:Characters",
	"https://dandadan.fandom.com/wiki/Category:Characters",
	"https://kaichouwamaidsama.fandom.com/wiki/Category:Characters",
	"https://ao-haru-ride.fandom.com/wiki/Category:Characters",
	"https://sayiloveyou.fandom.com/wiki/Category:Characters",
	"https://kiminitodoke.fandom.com/wiki/Category:Characters",
	"https://watashi-ga-motete-dousunda.fandom.com/wiki/Category:Characters",

    ]

    def start_requests(self):
        # ğŸ”¹ On envoie toutes les requÃªtes avec errback pour gÃ©rer les erreurs
        for url in self.start_urls:
            yield scrapy.Request(
                url, 
                callback=self.parse, 
                errback=self.errback_httpbin,
                dont_filter=True
            )

    def parse(self, response):
        # âœ… Page accessible
        print(f"ACCESSIBLE âœ… : {response.url}")

    def errback_httpbin(self, failure):
        # âŒ Page bloquÃ©e ou autre erreur
        request = failure.request
        print(f"BLOCKED âŒ : {request.url} - {repr(failure.value)}")
